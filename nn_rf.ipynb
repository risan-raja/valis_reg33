{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "import scipy\n",
    "import skimage\n",
    "import ants\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_ksize(sigma):\n",
    "    # opencv calculates ksize from sigma as\n",
    "    # sigma = 0.3*((ksize-1)*0.5 - 1) + 0.8\n",
    "    # then ksize from sigma is\n",
    "    # ksize = ((sigma - 0.8)/0.15) + 2.0\n",
    "\n",
    "    return int(((sigma - 0.8)/0.15) + 2.0)\n",
    "\n",
    "def get_gaussian_blur(img, ksize=0, sigma=5):\n",
    "    # if ksize == 0, then compute ksize from sigma\n",
    "    if ksize == 0:\n",
    "        ksize = get_ksize(sigma)\n",
    "\n",
    "    # Gaussian 2D-kernel can be seperable into 2-orthogonal vectors\n",
    "    # then compute full kernel by taking outer product or simply mul(V, V.T)\n",
    "    sep_k = cv2.getGaussianKernel(ksize, sigma)\n",
    "\n",
    "    # if ksize >= 11, then convolution is computed by applying fourier transform\n",
    "    return cv2.filter2D(img, -1, np.outer(sep_k, sep_k))\n",
    "\n",
    "def ssr(img, sigma):\n",
    "    # Single-scale retinex of an image\n",
    "    # SSR(x, y) = log(I(x, y)) - log(I(x, y)*F(x, y))\n",
    "    # F = surrounding function, here Gaussian\n",
    "\n",
    "    return np.log10(img) - np.log10(get_gaussian_blur(img, ksize=0, sigma=sigma) + 1.0)\n",
    "\n",
    "def msr(img, sigma_scales=[15, 80, 250]):\n",
    "    # Multi-scale retinex of an image\n",
    "    # MSR(x,y) = sum(weight[i]*SSR(x,y, scale[i])), i = {1..n} scales\n",
    "\n",
    "    msr = np.zeros(img.shape)\n",
    "    # for each sigma scale compute SSR\n",
    "    for sigma in sigma_scales:\n",
    "        msr += ssr(img, sigma)\n",
    "\n",
    "    # divide MSR by weights of each scale\n",
    "    # here we use equal weights\n",
    "    msr = msr / len(sigma_scales)\n",
    "\n",
    "    # computed MSR could be in range [-k, +l], k and l could be any real value\n",
    "    # so normalize the MSR image values in range [0, 255]\n",
    "    msr = cv2.normalize(msr, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8UC3)\n",
    "\n",
    "    return msr\n",
    "\n",
    "def color_balance(img, low_per, high_per):\n",
    "    '''Contrast stretch img by histogram equilization with black and white cap'''\n",
    "\n",
    "    tot_pix = img.shape[1] * img.shape[0]\n",
    "    # no.of pixels to black-out and white-out\n",
    "    low_count = tot_pix * low_per / 100\n",
    "    high_count = tot_pix * (100 - high_per) / 100\n",
    "\n",
    "    # channels of image\n",
    "    ch_list = []\n",
    "    if len(img.shape) == 2:\n",
    "        ch_list = [img]\n",
    "    else:\n",
    "        ch_list = cv2.split(img)\n",
    "\n",
    "    cs_img = []\n",
    "    # for each channel, apply contrast-stretch\n",
    "    for i in range(len(ch_list)):\n",
    "        ch = ch_list[i]\n",
    "        # cummulative histogram sum of channel\n",
    "        cum_hist_sum = np.cumsum(cv2.calcHist([ch], [0], None, [256], (0, 256)))\n",
    "\n",
    "        # find indices for blacking and whiting out pixels\n",
    "        li, hi = np.searchsorted(cum_hist_sum, (low_count, high_count))\n",
    "        if (li == hi):\n",
    "            cs_img.append(ch)\n",
    "            continue\n",
    "        # lut with min-max normalization for [0-255] bins\n",
    "        lut = np.array([0 if i < li\n",
    "                        else (255 if i > hi else round((i - li) / (hi - li) * 255))\n",
    "                        for i in np.arange(0, 256)], dtype = 'uint8')\n",
    "        # constrast-stretch channel\n",
    "        cs_ch = cv2.LUT(ch, lut)\n",
    "        cs_img.append(cs_ch)\n",
    "\n",
    "    if len(cs_img) == 1:\n",
    "        return np.squeeze(cs_img)\n",
    "    elif len(cs_img) > 1:\n",
    "        return cv2.merge(cs_img)\n",
    "    return None\n",
    "\n",
    "def msrcr(img, sigma_scales=[15, 80, 250], alpha=125, beta=46, G=192, b=-30, low_per=1, high_per=3):\n",
    "    # Multi-scale retinex with Color Restoration\n",
    "    # MSRCR(x,y) = G * [MSR(x,y)*CRF(x,y) - b], G=gain and b=offset\n",
    "    # CRF(x,y) = beta*[log(alpha*I(x,y) - log(I'(x,y))]\n",
    "    # I'(x,y) = sum(Ic(x,y)), c={0...k-1}, k=no.of channels\n",
    "\n",
    "    img = img.astype(np.float64) + 1.0\n",
    "    # Multi-scale retinex and don't normalize the output\n",
    "    msr_img = msr(img, sigma_scales)\n",
    "    # Color-restoration function\n",
    "    crf = beta * (np.log10(alpha * img) - np.log10(np.sum(img, axis=2, keepdims=True)))\n",
    "    # MSRCR\n",
    "    msrcr_ = G * (msr_img*crf - b)\n",
    "    # normalize MSRCR\n",
    "    msrcr_ = cv2.normalize(msrcr_, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8UC3) # type: ignore\n",
    "    # color balance the final MSRCR to flat the histogram distribution with tails on both sides\n",
    "    msrcr_ = color_balance(msrcr_, low_per, high_per)\n",
    "\n",
    "    return msrcr_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BFIWSlide:\n",
    "    def __init__(self, slide_path, key=None, is_ref=False):\n",
    "        self.slide_path = slide_path\n",
    "        self.key = key\n",
    "        self.img = cv2.imread(slide_path)\n",
    "        self.img = cv2.cvtColor(self.img, cv2.COLOR_BGR2RGB)\n",
    "        self.msr_img = None\n",
    "        self.msr_img_gray = None\n",
    "        self.mask = None\n",
    "        self.is_ref = is_ref\n",
    "        self.apply_msrcr()\n",
    "        self.get_mask()\n",
    "        self.apply_mask(self.mask)\n",
    "\n",
    "    def apply_msrcr(self):\n",
    "        self.msr_img = msrcr(self.img)\n",
    "        self.msr_img_gray = cv2.cvtColor(self.msr_img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    def apply_mask(self, mask):\n",
    "        self.temp_img = (np.ones_like(self.img) * 255).astype(np.uint8)\n",
    "        self.temp_img[mask == 1] = self.img[mask == 1]\n",
    "        self.img = self.temp_img\n",
    "        self.temp_img = (np.ones_like(self.img) * 255).astype(np.uint8)\n",
    "        self.temp_img[mask == 1] = self.msr_img[mask == 1]\n",
    "        self.msr_img = self.temp_img\n",
    "\n",
    "    def get_mask(self):\n",
    "        if self.msr_img_gray is None:\n",
    "            self.apply_msrcr()\n",
    "        sample_ants = ants.from_numpy(self.msr_img_gray)\n",
    "        self.mask = sample_ants.get_mask(cleanup=4).numpy().astype(np.uint8) # type: ignore\n",
    "        # return self.mask\n",
    "    \n",
    "    def apply_crop(self, crop):\n",
    "        self.img = self.img[crop[0]:crop[1], crop[2]:crop[3]]\n",
    "        self.msr_img = self.msr_img[crop[0]:crop[1], crop[2]:crop[3]]\n",
    "        self.mask = self.mask[crop[0]:crop[1], crop[2]:crop[3]]\n",
    "    \n",
    "    def get_block_contours(self):\n",
    "        contours, _ = cv2.findContours(self.mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "        self.block_contour=contours[0]\n",
    "        self.block_bbox =  cv2.boundingRect(self.block_contour)\n",
    "        self.block_crop = (self.block_bbox[1], self.block_bbox[1]+self.block_bbox[3], self.block_bbox[0], self.block_bbox[0]+self.block_bbox[2])\n",
    "\n",
    "    def apply_block_crop(self):\n",
    "        self.apply_crop(self.block_crop)\n",
    "\n",
    "def make_slide(slide_path, key=None, is_ref=False):\n",
    "    slide = BFIWSlide(slide_path, key, is_ref)\n",
    "    return {key: slide}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BFIWReg:\n",
    "    def __init__(self, src_dir, dest_dir, ref_idx) -> None:\n",
    "        self.src_dir = src_dir\n",
    "        self.dest_dir = dest_dir\n",
    "        imgs = os.listdir(self.src_dir)\n",
    "        regex = re.compile(r\".*-SE_(\\d+)_original.jpg\")\n",
    "        imgs = sorted(imgs, key=lambda x: int(regex.match(x).group(1)))\n",
    "        imgs_ordered = {}\n",
    "        for img in imgs:\n",
    "            section_num = int(regex.match(img).group(1))\n",
    "            section_id = str(section_num)\n",
    "            section_id_digits = len(section_id)\n",
    "            if section_id_digits <4:\n",
    "                section_id = '0'*(4-section_id_digits) + str(section_num)\n",
    "            imgs_ordered[section_id] = os.path.join(self.src_dir, img)\n",
    "        self.imgs = imgs_ordered\n",
    "        if ref_idx not in self.imgs:\n",
    "            raise ValueError(\"Reference index not found in the image list\")\n",
    "        # self.ref_slide = BFIWSlide(self.imgs[ref_idx], key=ref_idx, is_ref=True)\n",
    "        self.slides_ = Parallel(n_jobs=32)(delayed(make_slide)(img, key) for key, img in tqdm(self.imgs.items()))\n",
    "        self.slides = {}\n",
    "        for slide in self.slides_:\n",
    "            self.slides.update(slide)\n",
    "        self.ref_slide = self.slides[ref_idx]\n",
    "        self.ref_slide.is_ref = True\n",
    "        print(\"Applying Reference Slide Mask to all slides\")\n",
    "        for key, slide in tqdm(self.slides.items()):\n",
    "            if key == ref_idx:\n",
    "                continue\n",
    "            slide.apply_mask(self.ref_slide.mask)\n",
    "        self.ref_slide.get_block_contours()\n",
    "        self.ref_crop = self.ref_slide.block_crop\n",
    "        print(\"Applying Reference Slide Crop to all slides\")\n",
    "        for key, slide in tqdm(self.slides.items()):\n",
    "            slide.apply_crop(self.ref_crop) \n",
    "        print(\"Applying Own Slide Block Crop to all slides\")\n",
    "        for key, slide in tqdm(self.slides.items()):\n",
    "            slide.get_block_contours()\n",
    "            slide.apply_block_crop()\n",
    "    \n",
    "    def save_output(self):\n",
    "        print(\"Saving `img` and `msr_img` of all slides\")\n",
    "        if not os.path.exists(self.dest_dir):\n",
    "            os.makedirs(self.dest_dir, exist_ok=True)\n",
    "        if not os.path.exists(os.path.join(self.dest_dir, \"msrcr\")):\n",
    "            os.makedirs(os.path.join(self.dest_dir, \"msrcr\"), exist_ok=True)\n",
    "        if not os.path.exists(os.path.join(self.dest_dir, \"original\")):\n",
    "            os.makedirs(os.path.join(self.dest_dir, \"original\"), exist_ok=True)\n",
    "        for key, slide in tqdm(self.slides.items()):\n",
    "            cv2.imwrite(os.path.join(self.dest_dir, f\"original/{key}.jpg\"), cv2.cvtColor(slide.img, cv2.COLOR_RGB2BGR))\n",
    "            cv2.imwrite(os.path.join(self.dest_dir, f\"msrcr/{key}_msrcr.jpg\"), cv2.cvtColor(slide.msr_img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "# reg = BFIWReg('/storage/valis_reg/test_block', '/storage/valis_reg/test_block_out', '1606')\n",
    "# reg.save_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:00<00:00, 83.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Reference Slide Mask to all slides\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:11<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Reference Slide Crop to all slides\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:00<00:00, 111324.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Own Slide Block Crop to all slides\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:00<00:00, 114.51it/s]\n"
     ]
    }
   ],
   "source": [
    "reg = BFIWReg('/storage/valis_reg/train_block', '/storage/valis_reg/train_block_out', '1606')\n",
    "# reg.save_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving `img` and `msr_img` of all slides\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:01<00:00, 26.65it/s]\n"
     ]
    }
   ],
   "source": [
    "reg.save_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_img(image):\n",
    "    image = cv2.GaussianBlur(image, (9, 9), 0.8)\n",
    "    green_thresh = skimage.filters.threshold_otsu(image[:,:,1])\n",
    "    img_g = image[:,:,1] <= green_thresh\n",
    "    img_g = scipy.ndimage.binary_fill_holes(img_g)\n",
    "    img_g = scipy.ndimage.binary_closing(img_g, iterations=20)\n",
    "    # Dilation\n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    img_g = cv2.dilate(img_g.astype(np.uint8), kernel, iterations = 15)\n",
    "\n",
    "    contours, _ = cv2.findContours(img_g.astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "    contour = contours[0]\n",
    "    contour_mask = np.zeros_like(img_g)\n",
    "    contour_mask = cv2.fillPoly(contour_mask.astype(np.uint8), [contour], 1)\n",
    "    foreground = cv2.bitwise_and(image, image, mask=contour_mask)\n",
    "    background = cv2.bitwise_and(image, image, mask=1-contour_mask)\n",
    "    result = {\n",
    "        \"fg\" : foreground,\n",
    "        \"bg\" : background,\n",
    "        \"mask\" : contour_mask,\n",
    "        \"img_g\": img_g\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:05<00:00,  5.95it/s]\n"
     ]
    }
   ],
   "source": [
    "train_images = []\n",
    "label_masks  = []\n",
    "slide_masks = []\n",
    "for k, slide in tqdm(reg.slides.items()):\n",
    "    if k== '1606' or k =='1588':\n",
    "        continue\n",
    "    tissue_trans = transform_img(slide.msr_img)\n",
    "    train_images.append(slide.msr_img)\n",
    "    label_masks.append(tissue_trans['mask'])\n",
    "    slide_masks.append(slide.mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Training Data from existing Slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:09<00:00,  3.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# Pixelwise Classifier Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "\n",
    "\n",
    "X = np.array([])\n",
    "y = np.array([])\n",
    "for i in trange(len(train_images)):\n",
    "    image = train_images[i]\n",
    "    mask = label_masks[i]\n",
    "    slide_mask = slide_masks[i]\n",
    "    act_mask = mask+slide_mask\n",
    "    bg_mask = (act_mask==1).astype(np.uint8)\n",
    "    tissue_mask = (act_mask==2).astype(np.uint8)\n",
    "    img_center = np.array([image.shape[0]//2, image.shape[1]//2])\n",
    "    # bg_mask = bg_mask - mask\n",
    "    tissue_coords = np.argwhere(tissue_mask)\n",
    "    # tissue_coords = tissue_coords/(np.linalg.norm(tissue_coords- img_center, axis=1).reshape(-1, 1)+1e-5)\n",
    "    tissue_coords = tissue_coords-img_center\n",
    "    bg_coords = np.argwhere(bg_mask)\n",
    "    # bg_coords = bg_coords/(np.linalg.norm(bg_coords- img_center, axis=1).reshape(-1, 1)+1e-5)\n",
    "    bg_coords = bg_coords-img_center\n",
    "    tX_ = np.c_[image[mask==1], tissue_coords]\n",
    "    bgX_ = np.c_[image[bg_mask==1], bg_coords][::4]\n",
    "    ty_ = np.ones(tissue_coords.shape[0])\n",
    "    by_ = np.zeros(bg_coords.shape[0])[::4]\n",
    "    X_ = np.concatenate([tX_, bgX_], axis=0)\n",
    "    y_ = np.concatenate([ty_, by_], axis=0)\n",
    "    X = np.concatenate([X, X_], axis=0) if X.size else X_\n",
    "    y = np.concatenate([y, y_], axis=0) if y.size else y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion to CuPy for faster Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "\n",
    "X = cp.array(X)\n",
    "y = cp.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.ensemble import RandomForestClassifier as cuRF\n",
    "from cuml.model_selection import train_test_split as cu_train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = cu_train_test_split(X, y, test_size=0.1, shuffle=True,stratify=y)\n",
    "X_test_, X_val, y_test_, y_val = cu_train_test_split(X_test, y_test, test_size=0.3, shuffle=True,stratify=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cuRF(max_depth = 30, n_estimators = 150, n_streams=8, n_bins=256, max_batch_size=32768)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9925150871276855"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:  0.9925150871276855\n",
      "Test Accuracy:  0.9924948811531067\n"
     ]
    }
   ],
   "source": [
    "from cuml.metrics import accuracy_score as cu_accuracy\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "acc = cu_accuracy(y_val, y_pred)\n",
    "print(\"Validation Accuracy: \",acc)\n",
    "\n",
    "y_pred = model.predict(X_test_)\n",
    "acc = cu_accuracy(y_test_, y_pred)\n",
    "print(\"Test Accuracy: \",acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=150, n_jobs=-1, bootstrap=True, warm_start=True, oob_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=True,stratify=y)\n",
    "X_test_, X_val, y_test_, y_val = train_test_split(X_test, y_test, test_size=0.3, shuffle=True,stratify=y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=150, n_jobs=-1, warm_start=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=150, n_jobs=-1, warm_start=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_estimators=150, n_jobs=-1, warm_start=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {balanced_accuracy_score(y_test_, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.98      0.99    729705\n",
      "         1.0       0.99      1.00      0.99   1772312\n",
      "\n",
      "    accuracy                           0.99   2502017\n",
      "   macro avg       0.99      0.99      0.99   2502017\n",
      "weighted avg       0.99      0.99      0.99   2502017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_.get(),y_pred.get()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import treelite.sklearn\n",
    "\n",
    "rf_model = treelite.sklearn.import_model(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.serialize(\"rf_model.tl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.dump_as_json(pretty_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models.resnet import ResNet101_Weights, ResNet18_Weights\n",
    "\n",
    "class PixelWiseClassifierWithBackbone(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=1):\n",
    "        super(PixelWiseClassifierWithBackbone, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=3, padding=1),  # Adjust in_channels based on backbone\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.conv_layers.cuda()\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.upsample.cuda()\n",
    "        # Final 1x1 convolution for classification\n",
    "        self.classifier = nn.Conv2d(32, num_classes, kernel_size=1, device='cuda')\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.backbone(x)\n",
    "        x = self.conv_layers(x)\n",
    "        # print(x.shape)\n",
    "        x = self.upsample(x)\n",
    "        # print(x.shape)\n",
    "        x = self.classifier(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "model = PixelWiseClassifierWithBackbone(num_classes=1)  # Adjust num_classes as needed\n",
    "# Define transformations (adjust as needed)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create Training Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlideDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            label = self.transform(label)\n",
    "        return image, label\n",
    "\n",
    "train_dataset = SlideDataset(train_images, label_masks, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed6203231cd4995908b1f6a8ffc0fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.027 MB of 0.036 MB uploaded\\r'), FloatProgress(value=0.7506241036808838, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>bce_loss</td><td>██████████▂▁▂▂▁▂▁▁▁▁</td></tr><tr><td>dice_loss</td><td>█▁▅▁▅▅▅▆▁▁▁▅▁▁▆▁▇▁▁▃</td></tr><tr><td>focal_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃██▇</td></tr><tr><td>jaccard_loss</td><td>█▁▅▁▅▅▅▆▁▁▁▅▁▁▆▁▇▁▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>bce_loss</td><td>0.0081</td></tr><tr><td>dice_loss</td><td>0.99766</td></tr><tr><td>focal_loss</td><td>0.1736</td></tr><tr><td>jaccard_loss</td><td>0.99883</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">helpful-grass-7</strong> at: <a href='https://wandb.ai/risan-raja-iitm/PixelClassification/runs/vopl7bn3' target=\"_blank\">https://wandb.ai/risan-raja-iitm/PixelClassification/runs/vopl7bn3</a><br/> View project at: <a href='https://wandb.ai/risan-raja-iitm/PixelClassification' target=\"_blank\">https://wandb.ai/risan-raja-iitm/PixelClassification</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240904_200220-vopl7bn3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/valis_reg/wandb/run-20240904_205814-9fhlq5bk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/risan-raja-iitm/PixelClassification/runs/9fhlq5bk' target=\"_blank\">decent-lion-8</a></strong> to <a href='https://wandb.ai/risan-raja-iitm/PixelClassification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/risan-raja-iitm/PixelClassification' target=\"_blank\">https://wandb.ai/risan-raja-iitm/PixelClassification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/risan-raja-iitm/PixelClassification/runs/9fhlq5bk' target=\"_blank\">https://wandb.ai/risan-raja-iitm/PixelClassification/runs/9fhlq5bk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/risan-raja-iitm/PixelClassification/runs/9fhlq5bk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f0ca87af550>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"PixelClassification\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\":0.001,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"train_dataset\",\n",
    "    \"epochs\": 10,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch.losses import DiceLoss, JaccardLoss, FocalLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.677221417427063\n",
      "Epoch: 0, Loss: 0.012502966448664665\n",
      "Epoch: 0, Loss: 0.08447713404893875\n",
      "Epoch: 0, Loss: 0.055609047412872314\n",
      "Epoch: 1, Loss: 0.0834987461566925\n",
      "Epoch: 1, Loss: 0.1388983279466629\n",
      "Epoch: 1, Loss: 0.14266450703144073\n",
      "Epoch: 1, Loss: 0.0951019749045372\n",
      "Epoch: 2, Loss: 0.07106077671051025\n",
      "Epoch: 2, Loss: 0.13818633556365967\n",
      "Epoch: 2, Loss: 0.11204668879508972\n",
      "Epoch: 2, Loss: 0.09304032474756241\n",
      "Epoch: 3, Loss: 0.14275449514389038\n",
      "Epoch: 3, Loss: 0.12492752820253372\n",
      "Epoch: 3, Loss: 0.14033807814121246\n",
      "Epoch: 3, Loss: 0.09233985841274261\n",
      "Epoch: 4, Loss: 0.0600423738360405\n",
      "Epoch: 4, Loss: 0.11728644371032715\n",
      "Epoch: 4, Loss: 0.1173849031329155\n",
      "Epoch: 4, Loss: 0.05707762762904167\n",
      "Epoch: 5, Loss: 0.07104809582233429\n",
      "Epoch: 5, Loss: 0.11177341639995575\n",
      "Epoch: 5, Loss: 0.12416783720254898\n",
      "Epoch: 5, Loss: 0.09223911911249161\n",
      "Epoch: 6, Loss: 0.07449799031019211\n",
      "Epoch: 6, Loss: 0.07098931819200516\n",
      "Epoch: 6, Loss: 0.010639290325343609\n",
      "Epoch: 6, Loss: 0.010389912873506546\n",
      "Epoch: 7, Loss: 0.00893817562609911\n",
      "Epoch: 7, Loss: 0.0043137818574905396\n",
      "Epoch: 7, Loss: 0.008856485597789288\n",
      "Epoch: 7, Loss: 0.01008117850869894\n",
      "Epoch: 8, Loss: 0.0043097189627587795\n",
      "Epoch: 8, Loss: 0.008056920021772385\n",
      "Epoch: 8, Loss: 0.01034451648592949\n",
      "Epoch: 8, Loss: 0.009946468286216259\n",
      "Epoch: 9, Loss: 0.009948326274752617\n",
      "Epoch: 9, Loss: 0.006669142749160528\n",
      "Epoch: 9, Loss: 0.0061922562308609486\n",
      "Epoch: 9, Loss: 0.008846594952046871\n"
     ]
    }
   ],
   "source": [
    "from segmentation_models_pytorch.losses import DiceLoss, JaccardLoss, FocalLoss\n",
    "def train_model(model, train_loader, num_epochs=10,):\n",
    "    model.train()\n",
    "    criterion = nn.BCELoss()\n",
    "    dice = DiceLoss(mode='binary')\n",
    "    jaccard = JaccardLoss(mode='binary')\n",
    "    focal = FocalLoss(mode='binary')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            outputs = outputs.squeeze(1)\n",
    "            outputs = transforms.Resize(images[0].shape[1:])(outputs)\n",
    "            loss = criterion(outputs.unsqueeze(0), labels)\n",
    "            dice_loss = dice(outputs.unsqueeze(0), labels)\n",
    "            jaccard_loss = jaccard(outputs.unsqueeze(0), labels)\n",
    "            focal_loss = focal(outputs.unsqueeze(0), labels)\n",
    "            loss = criterion(outputs.unsqueeze(0), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "                \n",
    "        wandb.log({\n",
    "            \"dice_loss\": dice_loss.item(),\n",
    "            \"jaccard_loss\": jaccard_loss.item(),\n",
    "            \"focal_loss\": focal_loss.item(),\n",
    "            \"bce_loss\": loss.item()\n",
    "            })\n",
    "\n",
    "\n",
    "train_model(model, train_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image\n",
    "image = cv2.imread('train_block_out/msrcr/0507_msrcr.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = transform(image)\n",
    "image_input = image.unsqueeze(0)  # Add batch dimension\n",
    "# Forward pass\n",
    "image_input = image_input.cuda()\n",
    "with torch.no_grad():  # No need to track gradients during inference\n",
    "    output = model(image_input)\n",
    "    threshold = 0.5\n",
    "    output = (output > threshold).float()\n",
    "# output = output.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "\n",
    "# img = F.to_pil_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1598, 1842, 3)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.imread('train_block_out/msrcr/1426_msrcr.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1598, 1842])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.squeeze(0).shape\n",
    "image_input.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f15a95b5ba0>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(af)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
